{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Weekly project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_url(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "def scrape_tiki(url = 'https://tiki.vn/dien-thoai-may-tinh-bang/c1789?'):\n",
    "    soup = get_url(url)\n",
    "    items = soup.find_all('div',{\"class\": \"product-item\"})\n",
    "    data = []\n",
    "    for item in items:\n",
    "        try: \n",
    "            dic = {\"product_id\":\"\",\"seller_id\":\"\",\"title\":\"\",\"price\":\"\",\"image_url\":\"\"}\n",
    "            dic[\"product_id\"] = item[\"data-id\"]\n",
    "            dic[\"seller_id\"] = item[\"data-seller-product-id\"]\n",
    "            dic[\"title\"] = item[\"data-title\"]\n",
    "            dic[\"price\"] = item[\"data-price\"]\n",
    "            if item.find(\"span\",{\"class\":\"image\"}):\n",
    "                dic[\"image_url\"] = item.find(\"span\",{\"class\":\"image\"}).img[\"src\"]\n",
    "            data.append(dic)\n",
    "        except:\n",
    "            print(\"We got an error\")\n",
    "    \n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scrape_tiki()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = pd.DataFrame(data,columns = data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "def get_url(url):  # create a function to parse the url\n",
    "    \"\"\"Get parsed HTML from url\n",
    "      Input: url to the webpage\n",
    "      Output: Parsed HTML text of the webpage\n",
    "    \"\"\"\n",
    "    # Send GET request\n",
    "    r = requests.get(url)  \n",
    "    \n",
    "    # Parse HTML text\n",
    "    soup = BeautifulSoup(r.text, 'html.parser') \n",
    "    return soup\n",
    "\n",
    "def scrape_tiki_all(url = \"https://tiki.vn/dien-thoai-may-tinh-bang/c1789?\"):\n",
    "    \"\"\"Scrape listing info of phone cat of tiki.vn\n",
    "      Input: url of a cat. Default: https://tiki.vn/dien-thoai-may-tinh-bang/c1789?\n",
    "      Output: A list containing scraped data of listing items\n",
    "    \"\"\"\n",
    "    url_base = url\n",
    "    \n",
    "    # List containing data of all articles\n",
    "    data = []\n",
    "    \n",
    "    page = 1\n",
    "    \n",
    "    items = True\n",
    "    \n",
    "    # start to find target tags and extract the item infos, stop when no info of items is found\n",
    "    while items != []:\n",
    "        \n",
    "        # Get parsed HTML\n",
    "        soup = get_url(url)\n",
    "        \n",
    "        # Find all tags that contain required info\n",
    "        items = soup.find_all('div',{\"class\": \"product-item\"}) \n",
    "        \n",
    "        # Extract information of each tag\n",
    "        for item in items: \n",
    "            \n",
    "            # We use the try-except blocks to handle errors\n",
    "            try: \n",
    "                \n",
    "                # Each tag is dictionary containing the required information\n",
    "                dic = {\"product_id\":\"\",\"seller_id\":\"\",\"title\":\"\",\"price\":\"\",\"image_url\":\"\"}\n",
    "                dic[\"product_id\"] = item[\"data-id\"]\n",
    "                dic[\"seller_id\"] = item[\"data-seller-product-id\"]\n",
    "                dic[\"title\"] = item[\"data-title\"]\n",
    "                dic[\"price\"] = item[\"data-price\"]\n",
    "                \n",
    "                # There are some articles without img tag...\n",
    "                if item.find(\"span\",{\"class\":\"image\"}):\n",
    "                    dic[\"image_url\"] = item.find(\"span\",{\"class\":\"image\"}).img[\"src\"]\n",
    "                \n",
    "                # Append the dictionary to data list\n",
    "                data.append(dic)\n",
    "            except:\n",
    "                \n",
    "                 # Skip if error and print error message\n",
    "                print(\"We got an error\")\n",
    "        \n",
    "        # print out the page number and items to keep track\n",
    "        print(page, len(data))\n",
    "        \n",
    "        # increment page\n",
    "        page += 1\n",
    "        \n",
    "        # create the url of the next page\n",
    "        url = url_base + \"&page=\" +str(page)\n",
    "        \n",
    "        #  control the scraping speed\n",
    "        time.sleep(randint(1,4))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the scraper\n",
    "phone = scrape_tiki_all()\n",
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a DataFrame\n",
    "phone_product = pd.DataFrame(phone,columns = phone[0].keys())\n",
    "phone_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract to pickle\n",
    "phone_product.to_pickle(\"./phone_product.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract to csv\n",
    "phone_product.to_csv(\"./phone_product.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
